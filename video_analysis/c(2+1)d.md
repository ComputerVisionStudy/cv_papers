### A Closer Look at Spatiotemporal Convolutions for Action Recognition

#### 1. Introduction

​		本文构建了来年各种功能形式的空间-时序卷积。第一种形式称为mixed convolution（MC），其仅在网络的早期层中使用3D卷积，在顶部层使用2D卷积。这种设计背后的原理是运动建模是低级/中级操作，其可以通过网络的早期层的3D卷积实现，而这些中级运动特征上的空间信息（通过顶层的2D卷积实现）产生准确的行为识别。第二个变体是“(2+1)D”卷积块，其将3D卷积显示分解为两个独立的顺序的操作——一个2D空间卷积和一个1D时序卷积。这种分解的好处是：（1）这两个操作之间额外的非线性整流，与具有相同参数的完整3D卷积相比，这有效地加倍了两个操作之间的非线性整流，因此使模型能够代表更复杂的功能；（2）这种分解促进优化，产生更低的训练损失和测试损失。

#### 3. Convolutional residual blocks for video

![fig1](images/R(2+1)D/fig1.png)

##### 3.1. R2D: 2D convolutions over the entire clip

​		视频中2D CNN忽略视频中时序顺序，并以类似通道的方式对待$L$帧。因此，这些模型将4D张量$\mathbf{x}$调整为$3L \times H \times W$的3D张量。第$i$个残差块的输出$\mathbf{z}_i$也是一个3D张量，其尺寸为$N_i \times H_i \times W_i$（其中$N_i$表示应用到第$i$块的卷积滤波器的数量），$H_i$和$W_i$为空间维度。每个滤波器的是3D的，并且大小为$N_{i-1} \times d \times d$，其中$d$表示空间宽和高。尽管滤波器是3维的，但是它仅涉及前一个张量$\mathbf{z}_{i-1}$的空间维度。每个滤波器产生单通达输出，因此，R2D中的第一个卷积层会使视频的整个时间信息折叠在单通道特征图中，从而防止了任何时间推理在后续层中发生。这种CNN架构如图1（a）。

##### 3.2. f-R2D: 2D convolutions over frames

​		另一种2D CNN通过一系列2D卷积残差块来独立地处理$L$帧，即将相同的滤波器应用到全部的$L$帧。从而没有任何用于时序建模的卷积层和全局空间-时序池化层。

##### 3.3. R3D: 3D convolutions

​		3D CNN保留着时序信息，并将它传播到网络的层。这种情况下，张量$\mathbf{z}_i$是4D的，并且大小为$N_i \times L \times H_i \times W_i$，其中$N_i$为第$i$个块的滤波器数量。每个滤波器是4维的，大小为$N_{i-1} \times t \times d \times d$，其中$t$为滤波器的时序维度大小（本文中，使用$t=3$）。这种网络如图1（d）。

![table1](images/R(2+1)D/table1.png)

##### 3.4. MC$x$ and rMC$x$: mixed 3D-2D convolutions

​		一种假设是运动建模（即3D卷积）在早期层中可能特别有用，而在语义抽象的更高级别（后期层）中，则不需要运动或时间建模。因此，一种合理的架构可能是以3D卷积开始，并在顶层使用2D卷积。由于在这项工作中，我们考虑具有5组卷积的3D ResNets（R3D）（请参见表1），因此我们的第一个变体是用2D卷积替换第5组中的所有3D卷积。该变体表示为MC5（Mixed Convolutions）。第二种变体是在足4和5中使用2D卷积，称为MC4。按照这种方式也创建了MC2和MC3。这种类型的CNN如图1（b）。另一种假设是时间建模可能在更深层有更大的收益，而早期层用2D卷积捕获外观信息，因此构建了“Reversed” Mixed Convolutions，对应的称为rMC2、rMC3、rMC4和rMC5，这种结构件图1（c）。

![fig2](images/R(2+1)D/fig2.png)

##### 3.5. R(2+1)D: (2+1)D convolutions

​		另一种可能的理论是，完整的3D卷积可以更方便地通过2D卷积和1D卷积来近似，将空间和时间建模分解为两个单独的步骤。因此设计了称为R(2+1)D的网络，将$N_i$个3D卷积滤波器$N_{i-1} \times t \times d \times d$替换为包含$M_i$个大小为$N_{i-1}\times 1 \times d \times d$的2D卷积滤波器和$N_i$个大小为$M_i \times t \times 1 \times 1$的时间卷积滤波器。超参数$M_i$确定了中间子空间的维度，其中信号在空间和时间卷积之间传播，$M_i = \lfloor \frac{td^2N_{i-1}N_i}{d^2N_{i-1}+tN_i} \rfloor$，使得(2+1)D块中参数量与完整的3D卷积近似相等。这种空间-时间的分解可以用于任意的3D卷积层，分解见图2所示，如果3D卷积有空间或时间卷积步长，步长也会相应的用于空间或时间维度，见图1（e）。

​		R(2+1)D分解有两个优势：（1）尽管没有改变参数量，但是由于2D和1D卷积之间的额外的ReLU将非线性的数量加倍，这增加了网络的复杂度；（2）将3D卷积分割为独立的空间和时间部分使得训练更加容易，图3给出了训练过程损失的变化。表4给出了R（2+1）D、R3D、R2D和P3D在Sports-1M上的比较。

![fig3](./images/R(2+1)D/fig3.png)

![table4](images/R(2+1)D/table4.png)

#### 4. Experiments

##### 4.1. Experimental setup

**Network architectures.**  表1给出了本文考虑的两种R3D架构（3DResNets）。输入大小为$112 \times 112$，在 conv1 中使用步长为$1 \times 2 \times 2$的卷积，conv3_1、 conv4_1 和 conv5_1 中的卷积步长为$2 \times 2 \times 2$。

**Training and evaluation.**  在Kinetics上，将视频帧调整到$128 \times 171$，然后随机裁剪大小为$112 \times 112$的窗口以生成每个片段。训练时，从视频中随机采样$L$帧。模型采用两种设置：8帧片段和16帧片段。每个GPU使用32个片段的mini-batch。初始学习率设置为0.01，并每10个epoch除以10，使用10个epoch的warm-up。训练在45个epoch后完成。

##### 4.2. Comparison of spatiotemporal convolutions

![table2](./images/R(2+1)D/table2.png)

​		表2报告了Kinetics验证集上的准确率。图5为Kinetics验证上准确率和计算复杂度（FLOPs）的比较。

![fig4](images/R(2+1)D/fig4.png)

​		图3给出了（2+1）D卷积比3D卷积更好的理由。

![fig3](./images/R(2+1)D/fig3.png)

##### 4.3. Revisiting practices for video-level prediction

![fig5](./images/R(2+1)D/fig5.png)

![table3](./images/R(2+1)D/table3.png)

​		图5和表3给出R(2+1)D上的最佳输入长度。

##### 4.4. Action recognition with a 34-layer R(2+1)D net

**Datasets.**  Sports-1M。由于Sports-1M平均每个视频有5分钟，因此每个视频均匀采样100个片段，并聚合100个片段的准确率作为视频级的预测。

![table4](images/R(2+1)D/table4.png)

表5给出了Sports-1M上预训练的网络，在Kinetics上微调的结果。

![table5](images/R(2+1)D/table5.png)

表6为迁移到UCF-101和HMDB51上的结果。

![table6](images/R(2+1)D/table6.png)